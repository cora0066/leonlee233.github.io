<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>卷积神经网络CNN笔记 | Leon&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="1. 什么是卷积神经网络？基本概念：卷积神经网络CNN（Convolutional Neural Networks） 是一种深度神经网络，最早在上世纪六十年代就已经被提出和研究了，最有名的使用卷积神经网络的两篇论文是 1998年的LeNet-5和2012年的AlexNet">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络CNN笔记">
<meta property="og:url" content="https://leonlee233.github.io/2023/10/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Leon&#39;s blog">
<meta property="og:description" content="1. 什么是卷积神经网络？基本概念：卷积神经网络CNN（Convolutional Neural Networks） 是一种深度神经网络，最早在上世纪六十年代就已经被提出和研究了，最有名的使用卷积神经网络的两篇论文是 1998年的LeNet-5和2012年的AlexNet">
<meta property="og:locale">
<meta property="og:image" content="https://pic2.zhimg.com/v2-ad14f7b6430e297fbc73b9ea5bf8872d_b.webp">
<meta property="og:image" content="https://i.mji.rip/2023/10/27/b8bc755f1c3277dfa9c483a03fd1d416.png">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-2ce695fbd365c2be94521992d52ccefd_720w.webp">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-a47095dd0902990d387e21ae24e6f0b9_720w.webp">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/13575947-fc6905f0e3f688bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/format/webp">
<meta property="og:image" content="https://i.mji.rip/2023/10/28/14855182fc38f50dce1633c3a66d7ef0.png">
<meta property="article:published_time" content="2023-10-27T05:30:17.000Z">
<meta property="article:modified_time" content="2023-10-28T12:29:58.698Z">
<meta property="article:author" content="Leon Lee">
<meta property="article:tag" content="Literature Review">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic2.zhimg.com/v2-ad14f7b6430e297fbc73b9ea5bf8872d_b.webp">
<meta name="twitter:creator" content="@https:&#x2F;&#x2F;twitter.com&#x2F;leonwithleo">
  
    <link rel="alternate" href="/atom.xml" title="Leon's blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Leon&#39;s blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">主页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" target="_blank" rel="noopener" href="https://targaryenliu.com/">Gary</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Suche"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://leonlee233.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-卷积神经网络CNN笔记" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E7%AC%94%E8%AE%B0/" class="article-date">
  <time class="dt-published" datetime="2023-10-27T05:30:17.000Z" itemprop="datePublished">2023-10-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      卷积神经网络CNN笔记
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="1-什么是卷积神经网络？"><a href="#1-什么是卷积神经网络？" class="headerlink" title="1. 什么是卷积神经网络？"></a>1. 什么是卷积神经网络？</h2><p>基本概念：卷积神经网络CNN（Convolutional Neural Networks）</p>
<p>是一种深度神经网络，最早在上世纪六十年代就已经被提出和研究了，最有名的使用卷积神经网络的两篇论文是</p>
<p>1998年的<strong>LeNet-5</strong>和2012年的<strong>AlexNet</strong></p>
<span id="more"></span> 
<p>我们一般提到的CNN都由这么几个结构构成<strong>卷积层</strong>，<strong>池化层</strong>，<strong>全连接层</strong>，接下来我们会详细介绍每一个部分</p>
<h2 id="2-卷积神经网络的发展历程"><a href="#2-卷积神经网络的发展历程" class="headerlink" title="2. 卷积神经网络的发展历程"></a>2. 卷积神经网络的发展历程</h2><ul>
<li><p>早期的CNN模型（如LeNet-5）</p>
<p>  AlexNet是CNN被应用在现代的深度神经网络上的标志时间节点</p>
</li>
<li><p>现代的CNN模型（如AlexNet，VGG，ResNet，MobileNet等）</p>
</li>
</ul>
<h2 id="3-卷积神经网络的基本结构"><a href="#3-卷积神经网络的基本结构" class="headerlink" title="3. 卷积神经网络的基本结构"></a>3. 卷积神经网络的基本结构</h2><h3 id="3-1先验知识"><a href="#3-1先验知识" class="headerlink" title="3.1先验知识"></a>3.1先验知识</h3><p>首先我们得了解下什么是<strong>Padding</strong>，<strong>Stride</strong>，<strong>Channel</strong></p>
<h4 id="padding："><a href="#padding：" class="headerlink" title="padding："></a>padding：</h4><p>中文意思是<strong>填充</strong>，就是在输入CNN图片的外层，添加一定数量的像素（通常用0像素填充），使得输出的特征图能和输入相匹配</p>
<h4 id="Stride："><a href="#Stride：" class="headerlink" title="Stride："></a>Stride：</h4><p>中文意思是<strong>步长</strong>，指的是卷积核每次移动的步数，步长增加的时候，卷积核滑动的距离就更长，输出的数据尺寸就会变小，特征提取能力就会变弱</p>
<h4 id="Channel："><a href="#Channel：" class="headerlink" title="Channel："></a>Channel：</h4><p><strong>通道数</strong>，也可以称之为深度和特征图数量，是一层卷积层中的输出的特征图数量，因为每一个通道的卷积核都不相同，他的大小影响了特征提取能力和计算网络的复杂性，通道数越多计算越复杂，特征提取能力越强</p>
<h3 id="3-2卷积层（Convolutional-Layer）"><a href="#3-2卷积层（Convolutional-Layer）" class="headerlink" title="3.2卷积层（Convolutional Layer）"></a>3.2卷积层（Convolutional Layer）</h3><p>先来看个图<br><img src="https://pic2.zhimg.com/v2-ad14f7b6430e297fbc73b9ea5bf8872d_b.webp" alt="卷积计算过程"><br>这里我们解决两个问题<br>其中<strong>蓝色</strong>的部分是我们的输入<br><strong>绿色</strong>部分表示我们的<strong>卷积核</strong>正在做的卷积运算<br><strong>红色</strong>的部分使我们计算输出的<strong>Feature Map</strong></p>
<h4 id="什么是卷积核（Convolutional-Kernel）？"><a href="#什么是卷积核（Convolutional-Kernel）？" class="headerlink" title="什么是卷积核（Convolutional Kernel）？"></a>什么是<strong>卷积核（Convolutional Kernel）</strong>？</h4><p>上面的动图中，要不断和输入特征图做计算的我们称之为卷积核，例如图中就是一个<strong>3X3</strong>的单通道<strong>卷积核</strong></p>
<p>你在其他的地方也有的时候能看到<strong>Filter</strong>这个单词，我们称之为<strong>滤波器</strong>，因为通常面对图像的时候我们并不会只用一个卷积核对特征图进行计算，我们会使用<strong>一组卷积核</strong>，这一组卷积核我们就称之为<strong>滤波器</strong>(<strong>Filter</strong>)</p>
<p><img src="https://i.mji.rip/2023/10/27/b8bc755f1c3277dfa9c483a03fd1d416.png" alt="卷积核和滤波器的关系.png"></p>
<h4 id="怎么做卷积运算（Convolutional-Operation）？"><a href="#怎么做卷积运算（Convolutional-Operation）？" class="headerlink" title="怎么做卷积运算（Convolutional Operation）？"></a>怎么做<strong>卷积运算（Convolutional Operation）</strong>？</h4><p>在单通道卷积上我们直接将卷积核和输入特征图之间的相<strong>对应位置相乘</strong>，最后将<strong>得出来的数</strong>汇总<strong>相加</strong>即可，成为输出特征图的<strong>一个位置的结果</strong></p>
<p>在经过卷积计算之后，特征图的大小有可能会发生变化，这个地方公式如下</p>
<p>输入数据的维度为 $input_dim$<br>卷积核的大小为 $kernel_size$<br>步长为 $stride$<br>填充大小为 $padding$<br>则输出数据的维度 $output_dim$ </p>
<p>$output_dim &#x3D; (input_dim - kernel_size + 2*padding)&#x2F;stride + 1$</p>
<p>就拿上面的图举例输入5X5，卷积核3X3，步长为1，padding为0，输入数据的维度就是（5-3）&#x2F;1+1&#x3D;3</p>
<h3 id="3-3-池化层（Pooling-Layer）"><a href="#3-3-池化层（Pooling-Layer）" class="headerlink" title="3.3 池化层（Pooling Layer）"></a>3.3 池化层（Pooling Layer）</h3><p>是一种对Feature Map进行压缩的算法，可以提升模型的鲁棒性，加快运行速度</p>
<h4 id="3-3-1-最大池化（Max-Pooling）"><a href="#3-3-1-最大池化（Max-Pooling）" class="headerlink" title="3.3.1 最大池化（Max Pooling）"></a>3.3.1 最大池化（Max Pooling）</h4><p>可以看作为使用了一个2X2的filter，步长为2，分别取filter中<strong>最大的数字</strong>输出，这个就是MaxPooling</p>
<p><img src="https://pic2.zhimg.com/80/v2-2ce695fbd365c2be94521992d52ccefd_720w.webp" alt="最大池化"></p>
<h4 id="3-3-2-平均池化（Average-Pooling）"><a href="#3-3-2-平均池化（Average-Pooling）" class="headerlink" title="3.3.2 平均池化（Average Pooling）"></a>3.3.2 平均池化（Average Pooling）</h4><p>同理平均池化，可以看做一个filter内的所有元素取平均，再按照步长分步输出<br><img src="https://pic2.zhimg.com/80/v2-a47095dd0902990d387e21ae24e6f0b9_720w.webp" alt="平均池化"></p>
<h3 id="3-4-全连接层（Fully-Connected-Layer）"><a href="#3-4-全连接层（Fully-Connected-Layer）" class="headerlink" title="3.4 全连接层（Fully Connected Layer）"></a>3.4 全连接层（Fully Connected Layer）</h3><p>全连接层通常的作用是，将前两步计算得到结果，提取特征进行分类和回归，<br>包含<strong>输入层</strong>，<strong>隐含层</strong>和<strong>输出层</strong>，可以通过调整输入输出层的维度来调整全连接层的参数</p>
<p><img src="https://upload-images.jianshu.io/upload_images/13575947-fc6905f0e3f688bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/format/webp" alt="全连接层"></p>
<p>这里头的<strong>黄色</strong>的神经元构成的可以看做是<strong>输入层</strong><br>同理<strong>绿色</strong>的就是我们说的<strong>隐含层</strong><br><strong>蓝色</strong>的神经元可以看作为<strong>输出层</strong></p>
<p>其实<strong>全连接层</strong>是<strong>多层感知机</strong>的一种，每一个上下层的神经元之间都会用<strong>权重</strong>和<strong>偏置</strong>连接起来</p>
<p>Pytorch已经封装的很好了，可以直接输入Input_dimension和output_dimension就可以计算了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.Linear(input_size,output_size)</span><br></pre></td></tr></table></figure>


<p>这里有一个演示CNN的<strong>可视化</strong>网页，如果感兴趣的话可以去看看，了解一下<br><a target="_blank" rel="noopener" href="https://poloclub.github.io/cnn-explainer/"> <img src="https://i.mji.rip/2023/10/28/14855182fc38f50dce1633c3a66d7ef0.png" alt="14855182fc38f50dce1633c3a66d7ef0.png"></a></p>
<h2 id="4-卷积神经网络的应用场景"><a href="#4-卷积神经网络的应用场景" class="headerlink" title="4. 卷积神经网络的应用场景"></a>4. 卷积神经网络的应用场景</h2><pre><code>- 图像分类  
- 目标检测与定位  
- 语音识别  
- 自然语言处理等。  
</code></pre>
<h2 id="5-如何训练一个卷积神经网络？"><a href="#5-如何训练一个卷积神经网络？" class="headerlink" title="5. 如何训练一个卷积神经网络？"></a>5. 如何训练一个卷积神经网络？</h2><pre><code>- 数据预处理  
    - 数据增强（Data Augmentation）  
    - 数据归一化（Data Normalization）等。  
- 模型训练  
    - 前向传播（Forward Propagation）  
    - 反向传播（Backpropagation）  
    - 损失函数与优化器选择等。  
</code></pre>
<h2 id="6-如何优化卷积神经网络的性能？"><a href="#6-如何优化卷积神经网络的性能？" class="headerlink" title="6. 如何优化卷积神经网络的性能？"></a>6. 如何优化卷积神经网络的性能？</h2><pre><code>- 改进模型结构（如使用更深的网络，使用注意力机制等）  
- 数据增强（Data Augmentation）  
- 早停（Early Stopping）等。  
</code></pre>
<h2 id="7-卷积神经网络未来的研究方向"><a href="#7-卷积神经网络未来的研究方向" class="headerlink" title="7. 卷积神经网络未来的研究方向"></a>7. 卷积神经网络未来的研究方向</h2><pre><code>- 网络结构改进  
- 正则化与泛化性能提升  
- 半监督学习与自监督学习等。  
</code></pre>
<h2 id="8-结论"><a href="#8-结论" class="headerlink" title="8. 结论"></a>8. 结论</h2><pre><code>- CNN的优势与特点  
- CNN在各个领域的应用前景和挑战等。  
</code></pre>
<h2 id="9-参考文献"><a href="#9-参考文献" class="headerlink" title="9. 参考文献"></a>9. 参考文献</h2><pre><code>- 提供相关的学术论文、研究报告和工程实践案例等引用信息。
</code></pre>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://leonlee233.github.io/2023/10/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E7%AC%94%E8%AE%B0/" data-id="clonvu4s0000lgwukcyi41uyw" data-title="卷积神经网络CNN笔记" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Literature-Review/" rel="tag">Literature Review</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/28/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Self-attension%E7%AC%94%E8%AE%B0/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          自注意力机制Self-attension笔记
        
      </div>
    </a>
  
  
    <a href="/2023/10/26/DETR%E7%AC%94%E8%AE%B0/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">文章总结：DETR笔记</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Code-Practice/" rel="tag">Code Practice</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hide/" rel="tag">Hide</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Literature-Review/" rel="tag">Literature Review</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E5%BF%86%E5%BD%95/" rel="tag">回忆录</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A5%87%E6%80%AA%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/" rel="tag">奇怪的碎碎念</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/" rel="tag">学习心得</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%91%84%E5%BD%B1/" rel="tag">摄影</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%B8%E8%AE%B0/" rel="tag">游记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BE%8E%E5%9B%BE/" rel="tag">美图</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Code-Practice/" style="font-size: 10px;">Code Practice</a> <a href="/tags/Hide/" style="font-size: 20px;">Hide</a> <a href="/tags/Literature-Review/" style="font-size: 14px;">Literature Review</a> <a href="/tags/%E5%9B%9E%E5%BF%86%E5%BD%95/" style="font-size: 12px;">回忆录</a> <a href="/tags/%E5%A5%87%E6%80%AA%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/" style="font-size: 10px;">奇怪的碎碎念</a> <a href="/tags/%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/" style="font-size: 18px;">学习心得</a> <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">学习笔记</a> <a href="/tags/%E6%91%84%E5%BD%B1/" style="font-size: 14px;">摄影</a> <a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 16px;">教程</a> <a href="/tags/%E6%B8%B8%E8%AE%B0/" style="font-size: 12px;">游记</a> <a href="/tags/%E7%BE%8E%E5%9B%BE/" style="font-size: 10px;">美图</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/11/06/Deformable-DETR%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91/">Deformable-DETR中文翻译</a>
          </li>
        
          <li>
            <a href="/2023/11/02/pytorch-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/">pytorch-加载数据集</a>
          </li>
        
          <li>
            <a href="/2023/10/28/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Self-attension%E7%AC%94%E8%AE%B0/">自注意力机制Self-attension笔记</a>
          </li>
        
          <li>
            <a href="/2023/10/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E7%AC%94%E8%AE%B0/">卷积神经网络CNN笔记</a>
          </li>
        
          <li>
            <a href="/2023/10/26/DETR%E7%AC%94%E8%AE%B0/">文章总结：DETR笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Leon Lee<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">主页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a target="_blank" rel="noopener" href="https://targaryenliu.com/" class="mobile-nav-link">Gary</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>