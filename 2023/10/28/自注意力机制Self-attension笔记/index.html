<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>自注意力机制Self-attension笔记 | Leon&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="论文《Attention is All You Need》笔记目录">
<meta property="og:type" content="article">
<meta property="og:title" content="自注意力机制Self-attension笔记">
<meta property="og:url" content="https://leonlee233.github.io/2023/10/28/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Self-attension%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Leon&#39;s blog">
<meta property="og:description" content="论文《Attention is All You Need》笔记目录">
<meta property="og:locale">
<meta property="article:published_time" content="2023-10-28T12:04:20.000Z">
<meta property="article:modified_time" content="2023-11-07T05:22:14.288Z">
<meta property="article:author" content="Leon Lee">
<meta property="article:tag" content="Literature Review">
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@https:&#x2F;&#x2F;twitter.com&#x2F;leonwithleo">
  
    <link rel="alternate" href="/atom.xml" title="Leon's blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Leon&#39;s blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">主页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" target="_blank" rel="noopener" href="https://targaryenliu.com/">Gary</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Suche"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://leonlee233.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-自注意力机制Self-attension笔记" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/10/28/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Self-attension%E7%AC%94%E8%AE%B0/" class="article-date">
  <time class="dt-published" datetime="2023-10-28T12:04:20.000Z" itemprop="datePublished">2023-10-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      自注意力机制Self-attension笔记
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="论文《Attention-is-All-You-Need》笔记目录"><a href="#论文《Attention-is-All-You-Need》笔记目录" class="headerlink" title="论文《Attention is All You Need》笔记目录"></a>论文《Attention is All You Need》笔记目录</h1><span id="more"></span>

<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><pre><code>这篇文章的摘要主要介绍了一下他们提出了一种简单的基于注意力机制开发的架构网络
</code></pre>
<ul>
<li>1.1 研究背景与动机</li>
<li>1.2 传统序列模型的局限性</li>
<li>1.3 Transformer模型的引入与意义</li>
</ul>
<h2 id="2-Transformer模型概述"><a href="#2-Transformer模型概述" class="headerlink" title="2. Transformer模型概述"></a>2. Transformer模型概述</h2><ul>
<li>2.1 注意力机制（Attention Mechanism）</li>
<li>2.2 多头自注意力机制（Multi-Head Self Attention）</li>
<li>2.3 前馈神经网络（Feedforward Neural Network）</li>
<li>2.4 残差连接与层归一化</li>
</ul>
<h2 id="3-模型架构详解"><a href="#3-模型架构详解" class="headerlink" title="3. 模型架构详解"></a>3. 模型架构详解</h2><ul>
<li>3.1 编码器（Encoder）架构</li>
<li>3.2 解码器（Decoder）架构</li>
<li>3.3 位置编码（Positional Encoding）</li>
</ul>
<h2 id="4-自注意力机制的数学原理"><a href="#4-自注意力机制的数学原理" class="headerlink" title="4. 自注意力机制的数学原理"></a>4. 自注意力机制的数学原理</h2><ul>
<li>4.1 Scaled Dot-Product Attention</li>
<li>4.2 多头自注意力的计算</li>
<li>4.3 注意力掩码（Attention Masking）</li>
</ul>
<h2 id="5-训练过程与优化"><a href="#5-训练过程与优化" class="headerlink" title="5. 训练过程与优化"></a>5. 训练过程与优化</h2><ul>
<li>5.1 损失函数</li>
<li>5.2 学习率调度</li>
<li>5.3 正则化与Dropout</li>
</ul>
<h2 id="6-实验与结果"><a href="#6-实验与结果" class="headerlink" title="6. 实验与结果"></a>6. 实验与结果</h2><ul>
<li>6.1 数据集与预处理</li>
<li>6.2 实验设置</li>
<li>6.3 评估指标与实验结果</li>
</ul>
<h2 id="7-与传统模型的对比研究"><a href="#7-与传统模型的对比研究" class="headerlink" title="7. 与传统模型的对比研究"></a>7. 与传统模型的对比研究</h2><ul>
<li>7.1 与循环神经网络（RNN）的对比</li>
<li>7.2 与卷积神经网络（CNN）的对比</li>
</ul>
<h2 id="8-应用与拓展"><a href="#8-应用与拓展" class="headerlink" title="8. 应用与拓展"></a>8. 应用与拓展</h2><ul>
<li>8.1 机器翻译任务</li>
<li>8.2 文本生成任务</li>
<li>8.3 图像处理等领域的拓展</li>
</ul>
<h2 id="9-总结与展望"><a href="#9-总结与展望" class="headerlink" title="9. 总结与展望"></a>9. 总结与展望</h2><ul>
<li>9.1 主要研究成果总结</li>
<li>9.2 可能的未来研究方向</li>
</ul>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><ul>
<li>A. 实现细节与代码解析</li>
<li>B. Transformer模型的变体与衍生</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://leonlee233.github.io/2023/10/28/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Self-attension%E7%AC%94%E8%AE%B0/" data-id="clonvxnyj0019j8uk4lheeyfq" data-title="自注意力机制Self-attension笔记" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Literature-Review/" rel="tag">Literature Review</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/02/pytorch-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          pytorch-加载数据集
        
      </div>
    </a>
  
  
    <a href="/2023/10/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E7%AC%94%E8%AE%B0/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">卷积神经网络CNN笔记</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Code-Practice/" rel="tag">Code Practice</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hide/" rel="tag">Hide</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Literature-Review/" rel="tag">Literature Review</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E5%BF%86%E5%BD%95/" rel="tag">回忆录</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A5%87%E6%80%AA%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/" rel="tag">奇怪的碎碎念</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/" rel="tag">学习心得</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%91%84%E5%BD%B1/" rel="tag">摄影</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B8%B8%E8%AE%B0/" rel="tag">游记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BE%8E%E5%9B%BE/" rel="tag">美图</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Code-Practice/" style="font-size: 10px;">Code Practice</a> <a href="/tags/Hide/" style="font-size: 20px;">Hide</a> <a href="/tags/Literature-Review/" style="font-size: 14px;">Literature Review</a> <a href="/tags/%E5%9B%9E%E5%BF%86%E5%BD%95/" style="font-size: 12px;">回忆录</a> <a href="/tags/%E5%A5%87%E6%80%AA%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/" style="font-size: 10px;">奇怪的碎碎念</a> <a href="/tags/%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/" style="font-size: 18px;">学习心得</a> <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">学习笔记</a> <a href="/tags/%E6%91%84%E5%BD%B1/" style="font-size: 14px;">摄影</a> <a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 16px;">教程</a> <a href="/tags/%E6%B8%B8%E8%AE%B0/" style="font-size: 12px;">游记</a> <a href="/tags/%E7%BE%8E%E5%9B%BE/" style="font-size: 10px;">美图</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/11/06/Deformable-DETR%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91/">Deformable-DETR中文翻译</a>
          </li>
        
          <li>
            <a href="/2023/11/02/pytorch-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/">pytorch-加载数据集</a>
          </li>
        
          <li>
            <a href="/2023/10/28/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Self-attension%E7%AC%94%E8%AE%B0/">自注意力机制Self-attension笔记</a>
          </li>
        
          <li>
            <a href="/2023/10/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E7%AC%94%E8%AE%B0/">卷积神经网络CNN笔记</a>
          </li>
        
          <li>
            <a href="/2023/10/26/DETR%E7%AC%94%E8%AE%B0/">文章总结：DETR笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Leon Lee<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">主页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a target="_blank" rel="noopener" href="https://targaryenliu.com/" class="mobile-nav-link">Gary</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>